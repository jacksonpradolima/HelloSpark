Apache Spark is an open source cluster computing framework. Originally developed at the University of California,
Berkeley's AMPLab, the Spark codebase was later donated to the Apache Software Foundation that has maintained it
since. Spark provides an interface for programming entire clusters with implicit data parallelism and fault-tolerance.

Apache Spark provides programmers with an application programming interface centered on a data structure
called the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a
cluster of machines, that is maintained in a fault-tolerant way.[1] It was developed in response to
limitations in the MapReduce cluster computing paradigm, which forces a particular linear dataflow structure
on distributed programs: MapReduce programs read input data from disk, map a function across the data, reduce
the results of the map, and store reduction results on disk. Spark's RDDs function as a working set for distributed
programs that offers a (deliberately) restricted form of distributed shared memory.

The availability of RDDs facilitates the implementation of both iterative algorithms, that visit their dataset
multiple times in a loop, and interactive/exploratory data analysis, i.e., the repeated database-style querying
of data. The latency of such applications (compared to Apache Hadoop, a popular MapReduce implementation)
may be reduced by several orders of magnitude.[1][3] Among the class of iterative algorithms are the training
algorithms for machine learning systems, which formed the initial impetus for developing Apache Spark.

Apache Spark requires a cluster manager and a distributed storage system. For cluster management,
Spark supports standalone (native Spark cluster), Hadoop YARN, or Apache Mesos. For distributed storage,
Spark can interface with a wide variety, including Hadoop Distributed File System (HDFS),[6] MapR File System
(MapR-FS),[7] Cassandra,[8] OpenStack Swift, Amazon S3, Kudu, or a custom solution can be implemented.
Spark also supports a pseudo-distributed local mode, usually used only for development or testing purposes,
where distributed storage is not required and the local file system can be used instead; in such a scenario,
Spark is run on a single machine with one executor per CPU core.

Spark Core
Spark Core is the foundation of the overall project. It provides distributed task dispatching, scheduling,
and basic I/O functionalities, exposed through an application programming interface (for Java, Python, Scala, and R)
centered on the RDD abstraction. This interface mirrors a functional/higher-order model of programming: a "driver"
program invokes parallel operations such as map, filter or reduce on an RDD by passing a function to Spark,
which then schedules the function's execution in parallel on the cluster.[1] These operations, and additional ones
such as joins, take RDDs as input and produce new RDDs. RDDs are immutable and their operations are lazy;
fault-tolerance is achieved by keeping track of the "lineage" of each RDD, the sequence of operations produced it,
so that it can be reconstructed in the case of data loss. RDDs can contain any type of Python, Java, or Scala
objects.

Aside from the RDD-oriented functional style of programming, Spark provides two restricted forms of shared variables:
broadcast variables reference read-only data that needs to be available on all nodes, while accumulators can be
used to program reductions in an imperative style.

A typical example of RDD-centric functional programming is the following Scala program which computes the
frequencies of all words occurring in a set of text files and prints the most common ones.
Each map, flatMap (a variant of map) and reduceByKey takes an anonymous function that performs a simple operation
on a single data item (or a pair of items), and applies its argument to transform an RDD into a new RDD.

